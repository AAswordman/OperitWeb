# Beginner Guide: What Are AI Providers, Tokens, Endpoints, and Models?

If this is your first time configuring AI, terms like “provider,” “token,” “endpoint,” and “model” can feel confusing. A simple way to think about it is ordering food online: first you choose which restaurant to order from, and that is your AI provider; then you pay with your own account, and the credential that represents your identity is your token; next, your order must be sent to the correct store address, and that is the request endpoint; finally, you choose the exact dish and price level, and that is your model.

An AI provider is the platform that offers model services, such as OpenAI, Anthropic, Google, and other platforms compatible with OpenAI-style APIs. Operit AI does not create models by itself. It forwards your request to the provider you choose, then returns the response to you, which is why you need to register on a provider platform and create usable API credentials first.

A token is often called an API key. You can think of it as the key that tells the platform who you are and whose account should be billed. Your app sends this key with each request, and the provider uses it for authentication, permissions, usage tracking, and billing. Because it is sensitive, never share it publicly, never commit it to repositories, and avoid exposing it in screenshots. In most cases, you only need to configure it once in the app.

The endpoint is usually a URL, which is the entry point for API calls. You can think of it as the door your request must reach. If the endpoint is wrong or incompatible, requests fail even if your token is valid. Official providers usually publish a default endpoint, while some proxy or aggregation services publish their own compatible endpoints. For beginners, the safest approach is to start with the official documented endpoint, verify everything works, and then customize later.

The model determines capability, speed, and cost. In plain terms, different models are different engine tiers: some are faster and cheaper for everyday chat and light tasks, while others are stronger and more expensive for complex reasoning, long-context work, or high-quality generation. The model name you choose tells the provider which engine to run for this request, so changing the model can noticeably change output quality, latency, and price.

People often ask why API usage costs money, and why official apps sometimes look free. The key point is that “free to start” does not mean “zero cost.” Every model response consumes real infrastructure: GPUs, bandwidth, power, storage, operations, safety systems, and ongoing model development. Many official apps absorb part of that cost through product strategy, such as free-tier limits, slower response tiers, model restrictions, subscriptions, ecosystem subsidies, or business conversion. So the free app experience is usually an onboarding path, not unlimited long-term access to the highest service level.

Then why do advanced users still prefer API access? Because APIs are controllable, configurable, and integrable. You can choose models yourself, switch providers, tune parameters like temperature and context length, integrate AI into workflows, scripts, and products, and track usage and cost more transparently so you can optimize both quality and budget. For stable output, batch processing, or product integration, APIs are not the complicated option—they are the professional control panel.

If you are just getting started, pick an official entry-level model, keep default parameters, and run a few short tests first. Once your setup is stable and responsive, you can gradually move to stronger models. This gives you practical intuition about quality, latency, and spending, and helps avoid frustration from over-configuration too early.

Here are some commonly used provider entry points for quick registration and API setup: DeepSeek ([platform.deepseek.com](https://platform.deepseek.com/)), Moonshot/Kimi ([platform.moonshot.cn](https://platform.moonshot.cn/)), SiliconFlow ([cloud.siliconflow.cn](https://cloud.siliconflow.cn/)), OpenRouter ([openrouter.ai](https://openrouter.ai/)), and Zhipu ([open.bigmodel.cn](https://open.bigmodel.cn/)).